#!./.mnist-pytorch/bin/python
import os
from math import floor
import torch
import fire
from datasets import load_dataset , Dataset
from transformers import  AutoTokenizer
import pandas as pd



model_name='google/flan-t5-base'
tokenizer = AutoTokenizer.from_pretrained(model_name)

def splitset(dataset, parts):
    n = len(dataset)  # Total number of items in the dataset
    base_size = n // parts  # Minimum number of items per part
    remainder = n % parts  # Remaining items to distribute

    result = []
    start_idx = 0
    for i in range(parts):
        # Calculate the end index for this part, adding an extra item if there's remainder
        end_idx = start_idx + base_size + (1 if i < remainder else 0)
        result.append(dataset[start_idx:end_idx])
        start_idx = end_idx  # Update start index for the next part

    return result


def tokenize_function(example):
    start_prompt = 'Summarize the following conversation.\n\n'
    end_prompt = '\n\nSummary: '
    prompt = [start_prompt + dialogue + end_prompt for dialogue in example["dialogue"]]
    example['input_ids'] = tokenizer(prompt, padding="max_length", truncation=True, return_tensors="pt").input_ids
    example['labels'] = tokenizer(example["summary"], padding="max_length", truncation=True, return_tensors="pt").input_ids
    
    return example
def split(out_dir='data', n_splits=5):
    # Load the dataset
    dataset = "knkarthick/dialogsum"
    # Create the Dataset to create prompts.
    data = load_dataset(dataset,split='train[:50%]')
    # Convert to pandas DataFrame
    df = pd.DataFrame(data)

    # Shuffle the DataFrame
    df = df.sample(frac=1).reset_index(drop=True)

    # Convert back to datasets.Dataset
    shuffled_data = Dataset.from_pandas(df)

    tokenized_datasets = shuffled_data.map(tokenize_function, batched=True)
    #tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 10 == 0, with_indices=True)
    tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])
    

    # Size of each section
    n = len(tokenized_datasets)
    section_size = n // 5

    # Create 5 sections
    sections = []
    for i in range(5):
        subdir = f'{out_dir}/clients/{str(i+1)}'
        if not os.path.exists(subdir):
            os.makedirs(subdir)
        start_idx = i * section_size
        # For the last section, take all remaining data
        end_idx = (i + 1) * section_size if i != 4 else n
        section = tokenized_datasets.select(range(start_idx, end_idx))
        torch.save(section,f'{subdir}/dataset.pt')

if __name__ == '__main__':
    fire.Fire(split)
