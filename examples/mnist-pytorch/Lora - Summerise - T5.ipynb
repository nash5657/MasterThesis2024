{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df9f3152-54ba-4aa4-8eea-23085004c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b986983-fb2e-46b3-923c-4738d959bbef",
   "metadata": {},
   "source": [
    "Load the pre-trained FLAN-T5-base model and its tokenizer directly from HuggingFace. Notice that we will be using the small version of FLAN-T5. Setting torch_dtype=torch.bfloat16 specifies the memory type to be used by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b9a951-d7f8-4b8a-9a12-c69518b72588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba809ec0-f341-4b73-8844-a4dc573dafc3",
   "metadata": {},
   "source": [
    "Experimenting with the DialogSum Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1749cbf2-c8c5-4572-b955-6dcd71733008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb0165-c64b-4af9-b4bf-117b66ca43ec",
   "metadata": {},
   "source": [
    "You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with Summarize the following conversation and to the start of the summary with Summary as follows:\n",
    "\n",
    "Training prompt (dialogue):\n",
    "\n",
    "Summarize the following conversation.\n",
    "\n",
    "    Chris: This is his part of the conversation.\n",
    "    Antje: This is her part of the conversation.\n",
    "    \n",
    "Summary: \n",
    "Training response (summary):\n",
    "\n",
    "Both Chris and Antje participated in the conversation.\n",
    "Then preprocess the prompt-response dataset into tokens and pull out their input_ids (1 per token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df162a97-7997-4689-9e20-d338553f10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8dd16e3-7a0a-42da-9de2-c4940df0fee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a82d620-ccbe-4c10-a3d8-2fe801a594cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da9fa722-a3fd-4e34-8268-de5d0560c16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47068e48-7542-41a8-87cf-9c2ddd6eb977",
   "metadata": {},
   "source": [
    "fully fine-tuned version of the model we call it **instruct model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "209d9612-5d62-47db-a558-ee7024607e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('nash5657/flan-t5-instruct', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3977c5dd-a3df-45c3-a8ad-e931f72210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = AutoModelForSeq2SeqLM.from_pretrained('nash5657/t5-small-dataset', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef546e1f-b8d6-41f8-9a43-944456db6c42",
   "metadata": {},
   "source": [
    "set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fe2ce67-83f6-4c40-993a-bff1a6d05e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9acbe17-c12b-4977-982e-b9bcebad9b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3538944 || all params: 251116800 || trainable%: 1.4092820552029972\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a980b2d8-6858-4383-8a0d-e75d314b3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347d5276-d648-4bf3-b856-116d2d3b06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-1, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    #logging_steps=1e-1,\n",
    "    #max_steps=10   \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=small_tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7be6b-9cfb-43a4-b959-56c2df6342f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9/16 2:39:38 < 2:39:38, 0.00 it/s, Epoch 0.50/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df65204-3793-4c43-98df-bccf123a914f",
   "metadata": {},
   "source": [
    "Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ac5f3ef-5061-4c73-b215-8b2c5b4fd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer.model.save_pretrained('./test/peft_trainer_v/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f56c6030-b9f8-4b0d-ad48-a3d77f6ab689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_trainer.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1239180f-04ce-4b2c-a036-14e182e7c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "peft_model = PeftModel.from_pretrained(original_model, \n",
    "                                       './test/peft_trainer_v/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "647e4c36-5183-4d39-a81e-7e58d77552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "25ec81b2-b86d-4838-acf5-87d801f6bb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 251116800 || trainable%: 0.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9748194-fe49-451e-851a-f1a13a9143e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429773d1-055a-4cf7-88a7-3986a11afc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.23781826118922889, 'rouge2': 0.11402387041773232, 'rougeL': 0.21746611065159452, 'rougeLsum': 0.21637010614833194}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.3837300511435397, 'rouge2': 0.15814273845528468, 'rougeL': 0.2864151527401888, 'rougeLsum': 0.28696973739266163}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "        prompt = f\"\"\"\n",
    "    Summarize the following conversation.\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "        original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "        \n",
    "        peft_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "        peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "    \n",
    "    # Evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "    \n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "        predictions=peft_model_summaries,\n",
    "        references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af9b4b4-9add-4ddc-955c-2173ae4fa894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\n",
      "rouge1: 14.59%\n",
      "rouge2: 4.41%\n",
      "rougeL: 6.89%\n",
      "rougeLsum: 7.06%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54eec081-52da-42df-b2f0-12b389a05c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Person1#: I need to take a dictation for you.',\n",
       " '#Person1#: I need to take a dictation for you.',\n",
       " '#Person1#: I need to take a dictation for you.',\n",
       " 'The traffic jam at the Carrefour intersection is a problem.',\n",
       " 'The traffic jam at the Carrefour intersection is a problem.',\n",
       " 'The traffic jam at the Carrefour intersection is a problem.',\n",
       " 'Masha and Hero are getting divorced.',\n",
       " 'Masha and Hero are getting divorced.',\n",
       " 'Masha and Hero are getting divorced.',\n",
       " \"#Person1#: Happy birthday, Brian. #Person2#: I'm so happy you're having a good time. #Person1#: Thank you, I'm sure you're having a good time. #Person2#: Thank you, I'm sure you're having a good time. #Person1#: Thank you, I'm sure you're having a good time. #Person2#: Thank you, I'm sure you're having a good time. #Person1#: Thank you, I'm sure you're having a good time.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62732a69-dc55-49cc-8cd5-a93d87d09f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and Ms. Dawson says it applies to internal and external communications.',\n",
       " '#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and Ms. Dawson says it applies to internal and external communications.',\n",
       " '#Person1# asks Ms. Dawson to take a dictation to all employees by this afternoon. Ms. Dawson tells #Person1# all office communications are restricted to email correspondence and official memos. #Person1# wants to change the communication methods and Ms. Dawson says it applies to internal and external communications.',\n",
       " \"#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# thinks it's not good for #Person2# or for the environment.\",\n",
       " \"#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# thinks it's not good for #Person2# or for the environment.\",\n",
       " \"#Person2# got stuck in traffic again. #Person1# suggests #Person2# start taking public transport system to work and suggests #Person2# start biking to work when it's nicer outside. #Person2# thinks it's not good for #Person2# or for the environment.\",\n",
       " \"Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.\",\n",
       " \"Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.\",\n",
       " \"Masha and Hero are getting divorced. Kate can't believe it. #Person1# tells Kate they are having a separation for 2 months and filed for divorce. Kate thinks it's surprising and can't believe it.\",\n",
       " \"Brian's birthday is coming. #Person1# invites Brian to have a dance and Brian compliments #Person1#'s looks. Brian thinks #Person1# looks great and invites #Person1# to have a drink together.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab54af-7046-4b64-a7ba-a517f54a08dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (peft)",
   "language": "python",
   "name": "peft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
