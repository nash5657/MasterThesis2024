#!./.mnist-pytorch/bin/python
import collections
import math
import os

import docker
import fire
import torch

from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, AutoModelForCausalLM
import time
import evaluate
import pandas as pd
import numpy as np

import peft
from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig

import transformers


from fedn.utils.helpers.helpers import get_helper, save_metadata, save_metrics 

HELPER_MODULE = 'numpyhelper'
helper = get_helper(HELPER_MODULE)

def load_model():
    model_name='google/flan-t5-base'
    original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return original_model, tokenizer

def tokenize_function(example):
    #load the tokenizer
    tokenizer = load_model()[1] 

    start_prompt = 'Summarize the following conversation.\n\n'
    end_prompt = '\n\nSummary: '
    prompt = [start_prompt + dialogue + end_prompt for dialogue in example["dialogue"]]
    example['input_ids'] = tokenizer(prompt, padding="max_length", truncation=True, return_tensors="pt").input_ids
    example['labels'] = tokenizer(example["summary"], padding="max_length", truncation=True, return_tensors="pt").input_ids
    
    return example

def load_data():
    # Load the dataset
    dataset = "knkarthick/dialogsum"

    #Create the Dataset to create prompts.
    data = load_dataset(dataset)
    tokenized_datasets = data.map(tokenize_function, batched=True)
    tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])
    return tokenized_datasets["train"]

def save_parameters(model, out_path):
    """ Save model paramters to file.

    :param model: The model to serialize.
    :type model: torch.nn.Module
    :param out_path: The path to save to.
    :type out_path: str
    """
    parameters_np = [val.float().cpu().numpy() for _, val in model.state_dict().items()]
    helper.save(parameters_np, out_path)


def compile_model():
    peft_model_base = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base", torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")

    peft_model = PeftModel.from_pretrained(peft_model_base, 
                                        './peft-dialogue-summary-checkpoint-local/', 
                                        torch_dtype=torch.bfloat16,
                                        is_trainable=False)
    return peft_model, tokenizer

def load_parameters(model_path):
    """ Load model parameters from file and populate model.

    param model_path: The path to load from.
    :type model_path: str
    :return: The loaded model.
    :rtype: torch.nn.Module
    """
    parameters_np = helper.load(model_path)
    model = compile_model()[0]
    params_dict = zip(model.state_dict().keys(), parameters_np)
    state_dict = collections.OrderedDict({key: torch.tensor(x) for key, x in params_dict})
    model.load_state_dict(state_dict, strict=True)
    return model


def init_seed(out_path='seed.npz'):
    """ Initialize seed model and save it to file.

    :param out_path: The path to save the seed model to.
    :type out_path: str
    """
    # Init and save
    model = load_model()[0]
    save_parameters(model, out_path)


def train(in_model_path , out_model_path , data_path=None):

    # Train
    #output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'

    lora_config = LoraConfig(
    r=32, # Rank
    lora_alpha=32,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5
)

    peft_model = get_peft_model(load_model()[0], lora_config)

    working_dir = './'
    output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'

    peft_training_args = TrainingArguments(
        output_dir=output_dir,
        auto_find_batch_size=True,
        learning_rate=1e-3, # Higher learning rate than full fine-tuning.
        num_train_epochs=1,
        logging_steps=1,
        max_steps=1    
    )
    train_sample = load_data()

    trainer = Trainer(
        model=peft_model,
        args=peft_training_args,
        train_dataset=train_sample,
        data_collator=transformers.DataCollatorForLanguageModeling(load_model()[1], mlm=False)
    )

    trainer.train()

    # Metadata needed for aggregation server side
    metadata = {
        # num_examples are mandatory
         'num_examples': 1,
         'batch_size': 10,
         'epochs': 1,
         'lr': 0.01
        #'num_examples': len(train_sample),   
    }

    # Save JSON metadata file (mandatory)
    save_metadata(metadata, out_model_path)

    # print("Training finished")
    # trainer.save_model(output_directory)
    # print("hugging face Model saved")
    save_parameters(trainer.model, out_model_path)
    #print("fedn Model saved")

def validate(in_model_path, out_json_path, data_path=None):
    """ Validate model.

    :param in_model_path: The path to the input model.
    :type in_model_path: str
    :param out_json_path: The path to save the output JSON to.
    :type out_json_path: str
    :param data_path: The path to the data file.
    :type data_path: str
    """
    tokenizer = load_model()[1]
    original_model = load_model()[0]
    peft_model = compile_model()[0] 

    # Load data
    huggingface_dataset_name = "knkarthick/dialogsum"
    dataset = load_dataset(huggingface_dataset_name)
    dialogues = dataset['test'][0:10]['dialogue']
    human_baseline_summaries = dataset['test'][0:10]['summary']
    dialogues = dataset['test'][0:10]['dialogue']
    human_baseline_summaries = dataset['test'][0:10]['summary']

    original_model_summaries = []
    peft_model_summaries = []

    for idx, dialogue in enumerate(dialogues):
        prompt = f"""
    Summarize the following conversation.

    {dialogue}

    Summary: """
        
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids

        human_baseline_text_output = human_baseline_summaries[idx]
        
        original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))
        original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)

        peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))
        peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)

        original_model_summaries.append(original_model_text_output)
        peft_model_summaries.append(peft_model_text_output)

   
    # Load model
    model = load_parameters(in_model_path)
    

    # Evaluate
    criterion = torch.nn.NLLLoss()
    with torch.no_grad():
        train_out = model(x_train)
        training_loss = criterion(train_out, y_train)
        training_accuracy = torch.sum(torch.argmax(
            train_out, dim=1) == y_train) / len(train_out)
        test_out = model(x_test)
        test_loss = criterion(test_out, y_test)
        test_accuracy = torch.sum(torch.argmax(
            test_out, dim=1) == y_test) / len(test_out)

    # JSON schema
    report = {
        "training_loss": training_loss.item(),
        "training_accuracy": training_accuracy.item(),
        "test_loss": test_loss.item(),
        "test_accuracy": test_accuracy.item(),
    }

    # Save JSON
    save_metrics(report, out_json_path)

if __name__ == '__main__':
    fire.Fire({
        'init_seed': init_seed,
        'train': train,
    })
