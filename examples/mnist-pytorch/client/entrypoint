#!./.mnist-pytorch/bin/python
import collections
import math
import os

import docker
import fire
import torch

from datasets import load_dataset
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, AutoModelForCausalLM
import time
import evaluate
import pandas as pd
import numpy as np

import peft
from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig

import transformers
from datasets import Dataset

from transformers import T5Config
from transformers import T5ForConditionalGeneration



from fedn.utils.helpers.helpers import get_helper, save_metadata, save_metrics 

HELPER_MODULE = 'numpyhelper'
helper = get_helper(HELPER_MODULE)

def load_model():
    model_name='google/flan-t5-base'
    original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return original_model, tokenizer


def load_data(data_path):
    dataset = torch.load(data_path)
    #data = dataset.to_dict()
    return dataset
    
def save_parameters(model, out_path):
    """ Save model paramters to file.

    :param model: The model to serialize.
    :type model: torch.nn.Module
    :param out_path: The path to save to.
    :type out_path: str
    """
    # parameters_np = [val.float().cpu().numpy() for _, val in model.state_dict().items()]
    # helper.save(parameters_np, out_path)
    # Save configuration
    #model.config.to_json_file(out_path + '.json')

    # Save weights in NumPy format
    weights_dict = {k: v.float().cpu().numpy() for k, v in model.state_dict().items()}
    np.savez_compressed(out_path, **weights_dict)


# def compile_model():
#     peft_model_base = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base", torch_dtype=torch.bfloat16)
#     tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
#     instruct_model = 'nash5657/flan-t5-instruct'
#     peft_model = PeftModel.from_pretrained(peft_model_base, 
#                                         instruct_model , 
#                                         torch_dtype=torch.bfloat16,
#                                         is_trainable=False)
#     return peft_model, tokenizer

def load_parameters(model_path):
    """ Load model parameters from file and populate model.

    param model_path: The path to load from.
    :type model_path: str
    :return: The loaded model.
    :rtype: torch.nn.Module
    """
    parameters_np = helper.load(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base", torch_dtype=torch.bfloat16)
    # peft_model = PeftModel.from_pretrained(peft_model_base, 
    #                                     './peft-dialogue-summary-checkpoint-local', 
    #                                      torch_dtype=torch.bfloat16,
    #                                    is_trainable=False)
    params_dict = zip(model.state_dict().keys(), parameters_np)
    state_dict = collections.OrderedDict({key: torch.tensor(x) for key, x in params_dict})
    model.load_state_dict(state_dict, strict=True)
    # config = T5Config.from_pretrained("/Users/nash/Project/fedn/fedn/examples/mnist-pytorch/test/test.json")
    # model = T5ForConditionalGeneration(config=config)
    # a = np.load(model_path)
    # weights = []
    # for key in (a.files):
    #     weights.append(a[str(key)])
    # params_dict = zip(model.state_dict().keys(), weights)
    # state_dict = collections.OrderedDict({key: torch.tensor(x) for key, x in params_dict})
    # model.load_state_dict(state_dict, strict=True)
    return model


def init_seed(out_path='seed.npz'):
    """ Initialize seed model and save it to file.

    :param out_path: The path to save the seed model to.
    :type out_path: str
    """
    # Init and save
    model = load_model()[0]
    save_parameters(model, out_path)


def train(in_model_path , out_model_path , data_path):
    
    lora_config = LoraConfig(
    r=32, # Rank
    lora_alpha=32,
    target_modules=["q", "v"],
    lora_dropout=0.05,
    bias="lora_only",
    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5
)

    peft_model = get_peft_model(load_model()[0], lora_config)

    working_dir = './'
    output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'

    peft_training_args = TrainingArguments(
        output_dir=output_dir,
        auto_find_batch_size=True,
        learning_rate=1e-3, # Higher learning rate than full fine-tuning.
        num_train_epochs=4,
        #logging_steps=1,
        #max_steps=1   
    )
    train_sample = load_data(data_path)

    trainer = Trainer(
        model=peft_model,
        args=peft_training_args,
        train_dataset=train_sample,
        data_collator=transformers.DataCollatorForLanguageModeling(load_model()[1], mlm=False)
    )

    trainer.train()
    X = trainer.model.merge_and_unload()
    # Metadata needed for aggregation server side
    metadata = {
        # num_examples are mandatory
         'num_examples': len(train_sample),
         'batch_size': 10,
         'epochs': 1,
         'lr': 0.01
        #'num_examples': len(train_sample),   
    }

    # Save JSON metadata file (mandatory)
    save_metadata(metadata, out_model_path)

    # print("Training finished")
    # trainer.save_model(output_directory)
    # print("hugging face Model saved")
    save_parameters(X, out_model_path)
    #print("fedn Model saved")

def validate(in_model_path  , out_json_path  , data_path=None):
    """ Validate model.

    :param in_model_path: The path to the input model.
    :type in_model_path: str
    :param out_json_path: The path to save the output JSON to.
    :type out_json_path: str
    :param data_path: The path to the data file.
    :type data_path: str
    """
    model_name='google/flan-t5-base'
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    peft_model = load_parameters(in_model_path)

    # Load data
    #results = pd.read_csv("/app/data/rouge_dataset.csv")
    #dataset = pd.read_csv("/Users/nash/Project/fedn/fedn/examples/mnist-pytorch/rouge_dataset.csv")
    #huggingface_dataset_name = "knkarthick/dialogsum"
    
    dataset = pd.read_csv("/app/data/rouge_dataset.csv")
    dialogues = dataset['dialogue'][0:10]
    human_baseline_summaries = dataset['summary'][0:10]

    original_model_summaries = []
    instruct_model_summaries = []
    peft_model_summaries = []

    for idx, dialogue in enumerate(dialogues):
        prompt = f"""
    Summarize the following conversation.

    {dialogue}

    Summary: """
        
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids

        #human_baseline_text_output = human_baseline_summaries[idx]
        
        # original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))
        # original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)

        # instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))
        # instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)

        peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))
        peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)

        # original_model_summaries.append(original_model_text_output)
        # instruct_model_summaries.append(instruct_model_text_output)
        peft_model_summaries.append(peft_model_text_output)

    # zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))
    
    # df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])

        
    # Evaluate
    rouge = evaluate.load('rouge')
    # human_baseline_summaries = results['human_baseline_summaries'].values
    # #original_model_summaries = results['original_model_summaries'].values
    # #instruct_model_summaries = results['instruct_model_summaries'].values
    # peft_model_summaries     = results['peft_model_summaries'].values

    # original_model_results = rouge.compute(
    #     predictions=original_model_summaries,
    #     references=human_baseline_summaries[0:len(original_model_summaries)],
    #     use_aggregator=True,
    #     use_stemmer=True,
    # )

    # instruct_model_results = rouge.compute(
    #     predictions=instruct_model_summaries,
    #     references=human_baseline_summaries[0:len(instruct_model_summaries)],
    #     use_aggregator=True,
    #     use_stemmer=True,
    # )

    peft_model_results = rouge.compute(
        predictions=peft_model_summaries,
        references=human_baseline_summaries[0:len(peft_model_summaries)],
        use_aggregator=True,
        use_stemmer=True,
)
    # JSON schema
    report = {
        "rouge1": peft_model_results["rouge1"],
        "rouge2": peft_model_results["rouge2"],
        "rougeL": peft_model_results["rougeL"],
        "rougeLsum": peft_model_results["rougeLsum"],
    }

    # Save JSON
    save_metrics(report, out_json_path)

if __name__ == '__main__':
    fire.Fire({
        'init_seed': init_seed,
        'train': train,
        'validate': validate,
    })
